{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # system functions\n",
    "import os.path as op\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd, numpy as np\n",
    "# from functools import partial\n",
    "\n",
    "from nipype import config\n",
    "# config.enable_provenance()\n",
    "\n",
    "import nipype.interfaces.spm as spm\n",
    "import nipype.interfaces.spm.utils as spmu\n",
    "from nipype.interfaces.dcm2nii import Dcm2nii\n",
    "\n",
    "import nipype.interfaces.io as nio  # Data i/o\n",
    "import nipype.interfaces.utility as util  # utility\n",
    "import nipype.pipeline.engine as pe  # pypeline engine\n",
    "import nipype.algorithms.rapidart as ra  # artifact detection\n",
    "import nipype.algorithms.modelgen as model  # model specification\n",
    "import nipype.interfaces.matlab as mlab\n",
    "\n",
    "# TODO\n",
    "### BIDS-compatible naming\n",
    "### automated MATLAB and SPM pathfinding\n",
    "### model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBALS\n",
    "\n",
    "teeone = 'T1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt for path\n",
    "spm_path = ''\n",
    "matlab_path = ''\n",
    "\n",
    "if not spm.SPMCommand().version:    \n",
    "    while not op.isdir(spm_path):\n",
    "        spm_path = str(input('Please provide your SPM path:\\t'))\n",
    "        if not op.isdir(spm_path):\n",
    "            print('That\\'s not a valid path, let\\'s try again.')\n",
    "    while not op.isfile(matlab_path):\n",
    "        #print(matlab_path, '\\n', op.isfile(matlab_path), '\\n')\n",
    "        matlab_path = str(input('Please provide your Matlab path:\\t'))\n",
    "        if not op.isfile(matlab_path):\n",
    "            print('That\\'s not a valid path, let\\'s try again.')\n",
    "\n",
    "    matlab_path += ' -nodesktop -nosplash'\n",
    "\n",
    "#setup paths to SPM and matlab\n",
    "#if preferred, you can hardcode your SPM and matlab paths here\n",
    "spm.SPMCommand.set_mlab_paths(paths=spm_path, \n",
    "                              matlab_cmd=matlab_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE BACKEND FUNCTIONS\n",
    "#Assign spm procedures to preprocessing steps\n",
    "\n",
    "di = spmu.DicomImport()\n",
    "# use NewSegment in NiPyPe, Segment refers to an old routine (before SPM8)\n",
    "seg = spm.NewSegment()\n",
    "realign = spm.Realign()\n",
    "coreg = spm.Coregister()\n",
    "# Use Normalize12 as it corresponds to SPM12 routine\n",
    "norm12 = spm.Normalize12()\n",
    "smth = spm.Smooth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE FUNCTIONS\n",
    "\n",
    "def conversion(input_dir, output_dir):\n",
    "    di.inputs.in_files = input_dir\n",
    "    di.inputs.output_dir = output_dir\n",
    "    di.run() \n",
    "    \n",
    "    \n",
    "def segmentation(path_structural, deformation_fields=[False, True]):\n",
    "    \n",
    "    # Segmentation of T1 data, does not depend on functional files.\n",
    "    # Segment, Volumes = T1, Deformation field = forward (default)\n",
    "\n",
    "    #tissue maps\n",
    "    #path to TMP.nii\n",
    "    tpm_file = glob(op.join(spm_path, '**', 'tpm.nii'))[0]\n",
    "    tissue1 = ((tpm_file, 1), 2, (True, True), (False, False))\n",
    "    tissue2 = ((tpm_file, 2), 2, (True, True), (False, False))\n",
    "    tissue3 = ((tpm_file, 3), 2, (True, False), (False, False))\n",
    "    tissue4 = ((tpm_file, 4), 2, (False, False), (False, False))\n",
    "    tissue5 = ((tpm_file, 5), 2, (False, False), (False, False))\n",
    "    seg.inputs.tissues = [tissue1, tissue2, tissue3, tissue4, tissue5]\n",
    "    \n",
    "    seg.inputs.channel_files = path_structural\n",
    "    seg.inputs.write_deformation_fields = deformation_fields #Which deformation fields to write:[Inverse, Forward]\n",
    "    # ADD tissue maps\n",
    "    seg.run()\n",
    "    \n",
    "\n",
    "def realignment(input_files, rtm = True, jobtype = 'estwrite', quality = 0.9, fwhm = 5,\n",
    "                separation =4, interp = 2, wrap = [0, 0, 0], w_which = [2, 1],\n",
    "                w_interp = 4, w_wrap = [0, 0, 0], w_mask = True, prefix = 'r' ):\n",
    "    \n",
    "    # 1. REALIGNMENT\n",
    "    # Default: Estimate & Reslice, OUTPUT = realigned files (^.r)   \n",
    "    \n",
    "    realign.inputs.in_files = input_files\n",
    "    realign.inputs.register_to_mean = rtm\n",
    "    realign.inputs.jobtype = jobtype\n",
    "    realign.inputs.quality = quality\n",
    "    realign.inputs.fwhm = fwhm\n",
    "    realign.inputs.separation = separation\n",
    "    realign.inputs.interp = interp \n",
    "    realign.inputs.wrap = wrap\n",
    "\n",
    "    realign.inputs.write_which = w_which\n",
    "    realign.inputs.write_interp = w_interp\n",
    "    realign.inputs.write_wrap = w_wrap\n",
    "    realign.inputs.write_mask = w_mask\n",
    "    realign.inputs.out_prefix = prefix\n",
    "\n",
    "    realign.run()\n",
    "    \n",
    "    \n",
    "def coregistration(path_mean_functional, path_structural, jobtype = 'estimate', cost_f = 'nmi',\n",
    "                   fwhm = [7., 7.], separation = [4., 2.],\n",
    "                   tolerance = [0.02, 0.02, 0.02, 0.001, 0.001, 0.001, 0.01, 0.01, 0.01, 0.001, 0.001, 0.001]):\n",
    "    \n",
    "    # 2. COREGISTRATION\n",
    "    # Default: Coregister(Estimate), reference image = mean fMRI, source image = T1 \n",
    "\n",
    "    coreg.inputs.target = path_mean_functional #mean functional image\n",
    "    coreg.inputs.source = path_structural #structural image\n",
    "    coreg.inputs.jobtype = jobtype\n",
    "    coreg.inputs.cost_function = cost_f\n",
    "    coreg.inputs.fwhm = fwhm\n",
    "    coreg.inputs.separation = separation\n",
    "    coreg.inputs.tolerance = tolerance\n",
    "\n",
    "    coreg.run()\n",
    "    \n",
    "    \n",
    "def normalisation(path_y_s, path_realigned, jobtype = 'write', w_voxel_sizes = [3., 3., 3.],\n",
    "                  w_bounding_box = [[-78., -112.,  -70.],[78., 76., 85]], w_interp = 4,\n",
    "                  out_prefix = 'w'):\n",
    "    # 4. NORMALISATION\n",
    "    # Normalise(Write)\n",
    "    # Deformation field = y_s(T1)\n",
    "    # Images to write = realigned files (^r*)\n",
    "    # Default voxel size = [3 3 3]\n",
    "\n",
    "    norm12.inputs.deformation_file = path_y_s #deformation field\n",
    "    norm12.inputs.apply_to_files = path_realigned #functional realigned\n",
    "    norm12.inputs.jobtype = jobtype\n",
    "    norm12.inputs.write_voxel_sizes = w_voxel_sizes\n",
    "    norm12.inputs.write_bounding_box = w_bounding_box\n",
    "    norm12.inputs.write_interp = w_interp\n",
    "    norm12.inputs.out_prefix = out_prefix\n",
    "\n",
    "    norm12.run()    \n",
    "\n",
    "                                        \n",
    "def smoothing(path_normalised, fwhm = [6., 6., 6.], imp_masking = False, out_prefix = 's'):\n",
    "                                        \n",
    "    # 5. SMOOTHING\n",
    "    # Smooth, images to smooth = normalised files (^wr*)\n",
    "    # FWHM at least twice voxel size, default = [6 6 6]\n",
    "\n",
    "    smth.inputs.in_files = path_normalised\n",
    "    smth.inputs.fwhm = fwhm\n",
    "    smth.inputs.implicit_masking = imp_masking\n",
    "    smth.inputs.out_prefix = out_prefix\n",
    "\n",
    "    smth.run()\n",
    "                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an object with the commont path prefix\n",
    "prefix = os.getcwd()\n",
    "\n",
    "# extract list of study participants\n",
    "dirs_participants = [p for p in glob(op.join(prefix, '*')) if op.isdir(p)] #get all participants from a study\n",
    "participants = list(set([op.split(f)[-1] for f in dirs_participants])) #list with all participants from a study\n",
    "\n",
    "# extract list of timepoints\n",
    "dirs_tps = [p for p in glob(op.join(prefix, '*', '*'), recursive = True ) if op.isdir(p)]\n",
    "tps = list(set([op.split(f)[-1] for f in dirs_tps])) #list with all timepoints\n",
    "\n",
    "# extract list of tasks\n",
    "dirs_tasks = [p for p in glob(op.join(prefix, '*', '*', 'DICOM', '*'), recursive = True ) if op.isdir(p)]\n",
    "tasks =  list(set([op.split(f)[-1].split('-')[1] for f in dirs_tasks]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a nested dictionaty - data structure containing all pathnames\n",
    "# sorta like the old multi-index dataframe, but easier to navigate\n",
    "# Dictionary structure\n",
    "# dict_study: {participant: {timepoints: {tasks: {dicom_dir, nii_dir, tasktyoe}}}}\n",
    "\n",
    "dict_study = {participant:{} for participant in participants}\n",
    "\n",
    "# set to keep tasknames\n",
    "tasknames = set()\n",
    "\n",
    "keys = ['dicom_dir', 'nii_dir', 'tasktype']\n",
    "\n",
    "for participant in dict_study.keys():\n",
    "    for tp in tps:\n",
    "        dict_study[participant][tp] = {}\n",
    "        for task in tasks:\n",
    "            # CAUTION! Hardcoded: only a single DICOM dir per task. Needs improvement for multiple dirs present\n",
    "            dicom_dir = glob(op.join(prefix, participant, tp, 'DICOM', '*' + task + '*'), recursive = True)[0]\n",
    "            tasktype = task.split('_')[0]\n",
    "            # CAUTION! Hardcoded: previous knowledge about tasknames and procedure names.\n",
    "            # Needs improvement for better encoding of names\n",
    "            if 't1' in tasktype.lower():\n",
    "                taskname = teeone\n",
    "            else:\n",
    "                taskname = task.split(tasktype)[1][6:]\n",
    "            \n",
    "            tasknames.add(taskname)\n",
    "            \n",
    "            nii_dir = op.join(prefix, participant, tp, 'NIFTI', taskname)\n",
    "            dict_study[participant][tp][taskname] = {\n",
    "                keys[0] : dicom_dir, keys[1] : nii_dir, keys[2] : tasktype}\n",
    "\n",
    "tasknames = list(tasknames) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Paths to nifti files\n",
    "# not needed since they are defined in the dict_study population\n",
    "\n",
    "# cols = ['Participant', 'Timepoint', 'Task', 'Filepath']\n",
    "# fnii_paths = pd.DataFrame(columns = cols)\n",
    "# fstruct_paths =  pd.DataFrame(columns = cols)\n",
    "\n",
    "# timepoints = []\n",
    "# fMRI_tasks = []\n",
    "\n",
    "# for participant in dirs_participants:\n",
    "#     dirs_timepoints = [p for p in glob(op.join(participant, '*')) if op.isdir(p)]\n",
    "#     tps = list(set([op.split(f)[-1] for f in dirs_timepoints]))\n",
    "#     participant_name = op.split(participant)[-1].split('_')[1]\n",
    "#     for tp in tps:\n",
    "#         fls_tasks = [p for p in glob(op.join(participant, tp, 'NIFTI', '*')) if op.isdir(p)]\n",
    "#         tasks = tps = list(set([op.split(f)[-1] for f in fls_tasks]))\n",
    "#         if (tp not in timepoints):\n",
    "#             timepoints.append(tp)\n",
    "            \n",
    "#         for task in tasks:\n",
    "#             f_path = op.join(participant, tp, 'NIFTI', task)\n",
    "#             row = [participant_name, tp, task, f_path]\n",
    "#             if (task not in fMRI_tasks):\n",
    "#                 fMRI_tasks.append(task)\n",
    "                \n",
    "#             if any( t_one in task for t_one in ('t1', 'T1')):\n",
    "#                 fstruct_paths = fstruct_paths.append(pd.DataFrame([row], columns = cols), ignore_index = True)\n",
    "#             else:\n",
    "#                 fnii_paths = fnii_paths.append(pd.DataFrame([row], columns = cols), ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "#\n",
    "# CONVERT DICOM to .NII\n",
    "#\n",
    "#################################\n",
    "\n",
    "             \n",
    "for participant in participants:\n",
    "    for tp in tps:\n",
    "        for task in tasknames:\n",
    "            nii_path = dict_study[participant][tp][task][keys[1]]\n",
    "            # if a folder already exists\n",
    "            if op.isdir(nii_path):\n",
    "                # and is not empty\n",
    "                if len(os.listdir(nii_path)) > 0:\n",
    "                    ask = True\n",
    "                    while ask:\n",
    "                        #ask if files should be overwritten\n",
    "                        print('\\n', nii_path)\n",
    "                        q = str.lower(input('Files detected in target directory. Overwrite? - Y/N:\\t'))\n",
    "                        if any( answer == q for answer in ('y', 'yes', 'n', 'no')):\n",
    "                            ask = False\n",
    "                        if any( answer == q for answer in ('y', 'yes')):\n",
    "                            #convert\n",
    "                            conversion(glob(op.join(dict_study[participant][tp][task][keys[0]], '**', '*.dcm'), recursive = True),\n",
    "                                       nii_path) \n",
    "            # if a folder does not exist\n",
    "            else:\n",
    "                out_dir = Path(nii_path)\n",
    "                out_dir.mkdir(parents = True)\n",
    "                conversion(glob(op.join(dict_study[participant][tp][task][keys[0]], '**', '*.dcm'), recursive = True),\n",
    "                           nii_path)                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "#\n",
    "#  PREPROCESSING step by step\n",
    "#\n",
    "######################################\n",
    "\n",
    "# 3. SEGMENTATION\n",
    "# Segmentation of T1 data, does not depend on functional files.\n",
    "# Done once per timepoint\n",
    "#\n",
    "# Segment, Volumes = T1, Deformation field = forward\n",
    "# use NewSegment in NiPyPe, Segment refers to an old routine (before SPM8)\n",
    "\n",
    "\n",
    "for participant in participants:\n",
    "    for tp in tps:\n",
    "        # path to the structural file\n",
    "        segmentation(glob(op.join(dict_study[participant][tp][teeone][keys[1]], 's*.nii')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. REALIGNMENT\n",
    "# Estimate & Reslice, OUTPUT = realigned files (^.r)\n",
    "\n",
    "for participant in participants:\n",
    "    for tp in tps:\n",
    "        for task in tasknames:\n",
    "            if task != teeone:\n",
    "                realignment(glob(op.join(dict_study[participant][tp][task][keys[1]], 'f*.nii'))[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. COREGISTRATION\n",
    "# Coregister(Estimate), reference image = mean fMRI, source image = T1for participant in participants:\n",
    "\n",
    "for tp in tps:\n",
    "    for task in tasknames:\n",
    "        if task != teeone:\n",
    "            coregistration(glob(op.join(dict_study[participant][tp][task][keys[1]], 'mean*.nii'))[0], #mean functional image\n",
    "                           glob(op.join(dict_study[participant][tp][teeone][keys[1]], 's*.nii'))[0]) #structural image\n",
    "                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. NORMALISATION\n",
    "# Normalise(Write)\n",
    "# Deformation field = y_s(T1)\n",
    "# Images to write = realigned files (^r*)\n",
    "# Voxel size = [3 3 3]for participant in participants:\n",
    "\n",
    "for tp in tps:\n",
    "    for task in tasknames:\n",
    "        if task != teeone:\n",
    "            normalisation(glob(op.join(dict_study[participant][tp][teeone][keys[1]], 'y_s*.nii'))[0], #deformation field\n",
    "                          glob(op.join(dict_study[participant][tp][task][keys[1]], 'rf*.nii'))) #functional realigned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. SMOOTHING\n",
    "# Smooth, images to smooth = normalised files (^wr*)\n",
    "# FWHM at least twice voxel size\n",
    "\n",
    "for participant in participants:\n",
    "    for tp in tps:\n",
    "        for task in tasknames:\n",
    "            if task != teeone:\n",
    "                smoothing(glob(op.join(dict_study[participant][tp][task][keys[1]], 'wrf*.nii')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "#\n",
    "#  PREPROCESSING in one go\n",
    "#\n",
    "######################################\n",
    "\n",
    "for participant in participants:\n",
    "    for tp in tps:\n",
    "        segmentation(glob(op.join(dict_study[participant][tp][teeone][keys[1]], 's*.nii')))\n",
    "        for task in tasknames:\n",
    "            if task != teeone:\n",
    "                realignment(glob(op.join(dict_study[participant][tp][task][keys[1]], 'f*.nii'))[0])\n",
    "                coregistration(glob(op.join(dict_study[participant][tp][task][keys[1]], 'mean*.nii'))[0], #mean functional image\n",
    "                               glob(op.join(dict_study[participant][tp][teeone][keys[1]], 's*.nii'))[0]) #structural image\n",
    "                normalisation(glob(op.join(dict_study[participant][tp][teeone][keys[1]], 'y_s*.nii'))[0], #deformation field\n",
    "                              glob(op.join(dict_study[participant][tp][task][keys[1]], 'rf*.nii'))) #functional realigned\n",
    "                smoothing(glob(op.join(dict_study[participant][tp][task][keys[1]], 'wrf*.nii')))\n",
    "                               "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
